{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2005_intro_nb_ml.ipynb","provenance":[{"file_id":"https://github.com/random-forests/applied-dl/blob/master/examples/1.1-linear-regression.ipynb","timestamp":1560706781520}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gesTAALEnS7h","colab_type":"text"},"source":["# Introduction to TensorFlow\n","# Notebook - Machine Learning\n","\n","*Disclosure: This notebook is an adaptation of an official TensorFlow tutorial and Toronto's CSC421 tutorial material.*"]},{"cell_type":"markdown","metadata":{"id":"O4FyEVuoshl6","colab_type":"text"},"source":["##Installation\n","\n","Install any TensorFlow version via pip within the virtual machine provided by colab. If you are using the most recent stable version, no installation is required. The TensorFlow package is included by default within a colab.\n","\n","If you want to use a hardware accelerator (GPU), go to Runtime -> Change runtyme type -> Hardware accelerator -> GPU"]},{"cell_type":"code","metadata":{"id":"OBPIXNvlnnb7","colab_type":"code","colab":{}},"source":["# Install TF stable version or release candidate\n","!pip install tensorflow\n","\n","# Note: New package versions include CPU and GPU support\n","#       For releases 1.15 and older, CPU and GPU packages are separate (tensorflow-gpu)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pxt41JmyrRV1","colab_type":"code","colab":{}},"source":["# Import Tensorflow and NumPy\n","import tensorflow as tf  # regardless of cpu/gpu version , the module is always called tensorflow within python\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guhfkC_WrY4z","colab_type":"code","colab":{}},"source":["# Make sure correct version is installed\n","print(f\"Imported TensorFlow version: {tf.__version__}\")  # accessing the package version\n","print(f\"Imported TensorFlow version: {tf.version.VERSION}\")  # using the tf api"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wtulbijsNJU","colab_type":"text"},"source":["## Basic Operations\n","\n","Tensors can be stored in the graph as **constants** or **variables**. As you might guess, constants hold tensors whose values can't change, while variables hold tensors whose values can change. However, what you may not have guessed is that constants and variables are just more operations in the graph. A constant is an operation that always returns the same tensor value. A variable is an operation that will return whichever tensor has been assigned to it."]},{"cell_type":"code","metadata":{"id":"Bo5z-IbNsSIX","colab_type":"code","outputId":"6160f1fe-94b4-4e02-d595-aa88123bc56f","executionInfo":{"status":"ok","timestamp":1589104865914,"user_tz":-120,"elapsed":696,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["a = tf.constant([1, 2])\n","b = tf.constant([3, 4])\n","\n","c_1 = a+b\n","c_2 = tf.add(a, b)\n","\n","# Show result\n","print(c_1, c_2)\n","print('Tensor c', c_1, type(c_1))  # return Tensor\n","print('c.numpy', c_1.numpy(), type(c_1))  # return numpy n-d array view\n","\n","# Can be seamlessly used with numpy n-d arrays\n","d = np.eye(2, dtype=np.int32)  # identity matrix\n","\n","# numpy dot product of tf.tensor and numpy array\n","e = np.dot(c_1,d) \n","print('e', e, type(e))\n","\n","# tf tensor dot product of tf.tensor and numpy array\n","f = tf.tensordot(c_1,d, axes=1) \n","print('f', f, type(f))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensor c tf.Tensor([4 6], shape=(2,), dtype=int32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n","c.numpy [4 6] <class 'tensorflow.python.framework.ops.EagerTensor'>\n","e [4 6] <class 'numpy.ndarray'>\n","f tf.Tensor([4 6], shape=(2,), dtype=int32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cjqRMhFD7gnb","colab_type":"text"},"source":["## NumPy & TensorFlow n-d arrays"]},{"cell_type":"markdown","metadata":{"id":"wKgTZOEH-YXB","colab_type":"text"},"source":["###Same functionality"]},{"cell_type":"code","metadata":{"id":"kuJxn_em7loq","colab_type":"code","outputId":"6d47d2f8-0690-4fff-fbd8-95c60c9ac801","executionInfo":{"status":"ok","timestamp":1589104934071,"user_tz":-120,"elapsed":1087,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["# mostly similar commands\n","a = np.ones((2,2))\n","b = tf.ones((2,2))\n","\n","print(\"Numpy n-d array: \\n\", a)\n","print(\"TensorFlow Tensor: \\n\", b)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Numpy n-d array: \n"," [[1. 1.]\n"," [1. 1.]]\n","TensorFlow Tensor: \n"," tf.Tensor(\n","[[1. 1.]\n"," [1. 1.]], shape=(2, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9tBQmrDa-MAq","colab_type":"text"},"source":["###Similar functionality with different name"]},{"cell_type":"code","metadata":{"id":"cY0tOj0r9DnC","colab_type":"code","outputId":"ad0d3dbb-d5ab-43c2-bef0-2ce1cb447031","executionInfo":{"status":"ok","timestamp":1589104936893,"user_tz":-120,"elapsed":781,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["# some commands may be called different\n","a = np.ones((2,2))\n","b = tf.ones((2,2))\n","\n","c = np.sum(a)\n","d = tf.reduce_sum(b)\n","\n","print(\"Numpy n-d array: \\n\", c)\n","print(\"TensorFlow Tensor: \\n\", d)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Numpy n-d array: \n"," 4.0\n","TensorFlow Tensor: \n"," tf.Tensor(4.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-RMv_oD--h4g","colab_type":"text"},"source":["### Symbolic operators and broadcasting"]},{"cell_type":"code","metadata":{"id":"fIvxLA9A-my2","colab_type":"code","outputId":"fd630408-514b-4594-c8f8-f0c2c10e7e76","executionInfo":{"status":"ok","timestamp":1589104940119,"user_tz":-120,"elapsed":679,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["a = np.ones((2,2))\n","b = tf.ones((2,2))\n","\n","# element-wise multiplication + addition\n","a = a * 5 + 1\n","b = b * 5 + 1\n","\n","print(\"Numpy n-d array: \\n\", a)\n","print(\"TensorFlow Tensor: \\n\", b)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Numpy n-d array: \n"," [[6. 6.]\n"," [6. 6.]]\n","TensorFlow Tensor: \n"," tf.Tensor(\n","[[6. 6.]\n"," [6. 6.]], shape=(2, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6akeq6owCPXV","colab_type":"text"},"source":["###Indexing and slicing"]},{"cell_type":"code","metadata":{"id":"8FM51VkECTgz","colab_type":"code","outputId":"2aac0d80-e2f6-42f2-a6b2-d9d514378676","executionInfo":{"status":"ok","timestamp":1589104943027,"user_tz":-120,"elapsed":655,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["a = np.ones((2, 2))\n","b = tf.ones((2, 2))\n","\n","print(\"Scalar entry: \", a[0, 0])\n","print(\"First row: \", a[:, 0])\n","\n","print(\"Scalar entry: \", b[0, 0])\n","print(\"First row: \", b[:, 0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Scalar entry:  1.0\n","First row:  [1. 1.]\n","Scalar entry:  tf.Tensor(1.0, shape=(), dtype=float32)\n","First row:  tf.Tensor([1. 1.], shape=(2,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mm1VxpjPHN4o","colab_type":"text"},"source":["###Shape and rank"]},{"cell_type":"markdown","metadata":{"id":"q43g7t7ODxEl","colab_type":"text"},"source":["Numpy:"]},{"cell_type":"code","metadata":{"id":"JsyXrID6HRbY","colab_type":"code","outputId":"96848678-0003-4c86-ee7a-bcb009bcad49","executionInfo":{"status":"ok","timestamp":1589104946384,"user_tz":-120,"elapsed":1010,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["a = np.array([[1, 2, 3], [4, 5, 6]])\n","print(\"NumPy shape: \", a.shape)\n","print(\"NumPy ndim: \", a.ndim)  # Number of array dimensons"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NumPy shape:  (2, 3)\n","NumPy ndim:  2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yNGQQ8O_Dy6I","colab_type":"text"},"source":["It is crucial to build your data pipeline and networks with the right shapes, so have a look at the following examples for TensorFlow."]},{"cell_type":"code","metadata":{"id":"IVXu9c-4D0VJ","colab_type":"code","outputId":"9f03f611-7a49-400a-e433-01a73241d043","executionInfo":{"status":"ok","timestamp":1589104957715,"user_tz":-120,"elapsed":828,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["a = tf.constant(1)                               # scalar\n","b = tf.constant([1, 2, 3])                       # vector\n","c = tf.constant([[1, 2, 3], [4, 5, 6]])          # matrix\n","d = tf.constant([[[1, 2, 3]], [[7, 8, 9]]])      # n-d array\n","\n","\n","elements = [a, b, c, d]\n","\n","for idx, selected_element in enumerate(elements):\n","  print(\"Element Index: \", idx)\n","  print(\"TensorFlow Tensor: \", selected_element)\n","  print(\"TensorFlow Shape: \", selected_element.shape)  # Attribute\n","  print(\"TensorFlow Rank: \", tf.rank(selected_element))  # Function that returns number of array dimensions - not the rank of a matrix\n","  print(\"---------------------\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Element Index:  0\n","TensorFlow Tensor:  tf.Tensor(1, shape=(), dtype=int32)\n","TensorFlow Shape:  ()\n","TensorFlow Rank:  tf.Tensor(0, shape=(), dtype=int32)\n","---------------------\n","Element Index:  1\n","TensorFlow Tensor:  tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n","TensorFlow Shape:  (3,)\n","TensorFlow Rank:  tf.Tensor(1, shape=(), dtype=int32)\n","---------------------\n","Element Index:  2\n","TensorFlow Tensor:  tf.Tensor(\n","[[1 2 3]\n"," [4 5 6]], shape=(2, 3), dtype=int32)\n","TensorFlow Shape:  (2, 3)\n","TensorFlow Rank:  tf.Tensor(2, shape=(), dtype=int32)\n","---------------------\n","Element Index:  3\n","TensorFlow Tensor:  tf.Tensor(\n","[[[1 2 3]]\n","\n"," [[7 8 9]]], shape=(2, 1, 3), dtype=int32)\n","TensorFlow Shape:  (2, 1, 3)\n","TensorFlow Rank:  tf.Tensor(3, shape=(), dtype=int32)\n","---------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lIWolSz2c475","colab_type":"text"},"source":["## Automatic Differentiation with TensorFlow\n","Like Autograd, TensorFlow records all operations executed to a tape. It is commonly performed within a python context.\n","    x = tf.ones((2, 2))\n","\n","    with tf.GradientTape() as tape:\n","      tape.watch(x)\n","\n","The Gradient Tape will be used in this notebook, and will be revisited in Prart II of the Tensorflow Introduction. For more information, see https://www.tensorflow.org/tutorials/customization/autodiff\n"]},{"cell_type":"markdown","metadata":{"id":"nG2GQRD78oOd","colab_type":"text"},"source":["# Examples - Linear, Polynomial, Non-linear Regression\n","The next three sections of the notebook show examples of using TensorFlow in the context of three problems:\n","\n","1. **1-D linear regression**, where we try to fit a model to a function $y = wx + b$\n","2. **Linear regression using a polynomial feature map**, to fit a function of the form $y = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M$\n","3. **Nonlinear regression using a simple Neural Network**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SbB2-BmK1Kdw","colab_type":"text"},"source":["## Linear Regression with TensorFlow\n","\n","**Note:** This is a modified example of the google colab linear_regression.ipynb\n","\n","**Assumption:** Data distribution follows a linear model $$\n","y_i = wx_i + b + \\epsilon = 2 x_i + 0.5 + \\epsilon\n","$$\n","where $\\epsilon \\sim \\mathcal{N}(0, 0.001)$. \n","\n","A minimal example of linear regression in TensorFlow 2.0, written from scratch.  \n","\n","We'll create a few points on a scatter plot, then find the best fit line,  solving for $C(w, b) = \\frac{1}{N} \\sum\\limits_{i=1}^N (y_i - (w x_i + b))^2$ within TensorFlow."]},{"cell_type":"code","metadata":{"id":"x8doDs0i5oOU","colab_type":"code","colab":{}},"source":["# Import plotting functionality\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2n5vd8i5yCg","colab_type":"text"},"source":["Create a noisy dataset that's roughly linear, according to the equation y = w * x + b + noise."]},{"cell_type":"code","metadata":{"id":"mekZMKUW5xQj","colab_type":"code","colab":{}},"source":["def make_linear_data(w=2.0, b=0.5, N=100):\n","  x = tf.random.uniform(shape=(N,))  # TensorFlow and NumPy provide similar functionality\n","  eps = tf.random.normal(shape=(len(x),), stddev=0.1)\n","  y = w * x + b + eps\n","  return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jsb2XBE5TPNY","colab_type":"code","colab":{}},"source":["x_train, y_train = make_linear_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tSNaq8W52A2","colab_type":"code","colab":{}},"source":["plt.plot(x_train, y_train, 'r.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpzVrkNhI3y5","colab_type":"text"},"source":["Define trainable variables for our model. "]},{"cell_type":"code","metadata":{"id":"mwoChE9N5-aE","colab_type":"code","colab":{}},"source":["# Create trainable variables\n","w = tf.Variable(tf.random.normal(()))\n","b = tf.Variable(tf.random.normal(()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SEZq4GhJV5d","colab_type":"text"},"source":["Predict y given x."]},{"cell_type":"code","metadata":{"id":"TslVztFY7uQT","colab_type":"code","colab":{}},"source":["def predict(x):\n","  y = w * x + b\n","  return y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJW-r9JEJi1i","colab_type":"text"},"source":["Our loss will be the squared difference between the predicted values and the true values."]},{"cell_type":"code","metadata":{"id":"eTmFCjZX7JGV","colab_type":"code","colab":{}},"source":["def squared_error(y_pred, y_true):\n","  err = tf.reduce_mean(tf.square(y_pred - y_true)) \n","  return err"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVDFLZBrBikA","colab_type":"text"},"source":["Calculate loss before training."]},{"cell_type":"code","metadata":{"id":"25EhTz5QBkYS","colab_type":"code","colab":{}},"source":["loss = squared_error(predict(x_train), y_train)\n","print(\"Starting loss\", loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjHGdac1EWj6","colab_type":"text"},"source":["Use gradient descent to gradually improve our guess for `m` and `b`. At each step, we'll nudge them a little bit in the right direction to reduce the loss."]},{"cell_type":"code","metadata":{"id":"Qp6QFVQD6xB_","colab_type":"code","colab":{}},"source":["alpha = 0.025  # learning rate\n","steps = 2000\n","\n","for i in range(steps):\n","  # GradientTape fetches all variables needed for gradient calculation\n","  # Discussed in Part II\n","  with tf.GradientTape() as tape:\n","    predictions = predict(x_train)\n","    loss = squared_error(predictions, y_train)\n","\n","  # Calculate gradients via tape\n","  gradients = tape.gradient(loss, [w, b])\n","  \n","  # Manual gradient descent of w and b\n","  w.assign_sub(gradients[0] * alpha)\n","  b.assign_sub(gradients[1] * alpha)\n","  \n","  if i % 50 == 0:    \n","    print(f\"Step {i}, Loss {loss.numpy()}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4lPk7ZC87kU","colab_type":"text"},"source":["The learned values for m and b."]},{"cell_type":"code","metadata":{"id":"qVsdtuyp6-aP","colab_type":"code","colab":{}},"source":["print (f\"w: {w.numpy()}, b: {b.numpy()}\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ra6bq2rV_19Z","colab_type":"text"},"source":["Plot the best fit line."]},{"cell_type":"code","metadata":{"id":"rBvBWPgO_3wy","colab_type":"code","colab":{}},"source":["plt.plot(x_train, y_train, 'r.')\n","plt.plot(x_train, predict(x_train), 'b.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIikRd5kDQSp","colab_type":"text"},"source":["A couple things you can explore:\n","\n","* To understand gradient descent, try printing out the `gradients` calculated below. See how they're used to adjust the variables (`w` and `b`).\n","\n","* You can use TF 2.0 a lot like NumPy.  Try printing out the training data we created (`x_train`, `y_train`) and understand the format. Next, do the same for the variables (w and b). Notice both of these can be converted to NumPy format (with `.numpy()`)."]},{"cell_type":"markdown","metadata":{"id":"zJ17StfK6-pC","colab_type":"text"},"source":["### Bonus\n","Let's visualize the error surface as a function of w and b. This section is included purely for fun, you can skip it without missing anything."]},{"cell_type":"code","metadata":{"id":"jEQ_D8iP7fkf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":398},"outputId":"72394f2c-784c-438f-a2f4-261076d8be4b","executionInfo":{"status":"error","timestamp":1589107355719,"user_tz":-120,"elapsed":990,"user":{"displayName":"Marc Fischer","photoUrl":"","userId":"13738362924469851898"}}},"source":["# Warning: hacky code ahead\n","\n","import numpy as np\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# To plot the error surface, we'll need to get the loss\n","# for a bunch of different values for w and b.\n","\n","ws = np.linspace(-3, 3)\n","bs = np.linspace(-3, 3)\n","w_mesh, b_mesh = np.meshgrid(ws, bs)\n","\n","def loss_for_values(m, b):\n","  y = w * x_train + b\n","  loss = squared_error(y, y_train)\n","  return loss\n","\n","zs = np.array([loss_for_values(w, b) for (w,b) in zip(np.ravel(w_mesh), \n","                                                      np.ravel(b_mesh))])\n","z_mesh = zs.reshape(w_mesh.shape)\n","\n","fig = plt.figure(figsize=(12, 12))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(w_mesh, b_mesh, z_mesh, color='b', alpha=0.06)\n","\n","# At this point we have an error surface. \n","# Now we'll need a history of the gradient descent steps.\n","# So as not to complicate the above code,\n","# let's retrain the model here, keeping\n","# track of w, b, and loss at each step.\n","\n","# Intentionally start with this guess to \n","# make the plot nicer\n","w = tf.Variable(-.5)\n","b = tf.Variable(-.75)\n","\n","history = []\n","\n","for i in range(steps):\n","  with tf.GradientTape() as tape:\n","    predictions = predict(x_train)\n","    loss = squared_error(predictions, y_train)\n","  gradients = tape.gradient(loss, [w, b])\n","  history.append((w.numpy(), b.numpy(), loss.numpy()))\n","  w.assign_sub(gradients[0] * learning_rate)\n","  b.assign_sub(gradients[1] * learning_rate)\n","\n","# Plot the trajectory\n","ax.plot([h[0] for h in history], \n","        [h[1] for h in history], \n","        [h[2] for h in history],\n","        marker='o')\n","\n","ax.set_xlabel('w', fontsize=18, labelpad=20)\n","ax.set_ylabel('b', fontsize=18, labelpad=20)\n","ax.set_zlabel('loss', fontsize=18, labelpad=20)\n","\n","ax.view_init(elev=22, azim=28)"],"execution_count":19,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-66596321219e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m zs = np.array([loss_for_values(w, b) for (w,b) in zip(np.ravel(w_mesh), \n\u001b[0;32m---> 18\u001b[0;31m                                                       np.ravel(b_mesh))])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mz_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-66596321219e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m zs = np.array([loss_for_values(w, b) for (w,b) in zip(np.ravel(w_mesh), \n\u001b[0m\u001b[1;32m     18\u001b[0m                                                       np.ravel(b_mesh))])\n\u001b[1;32m     19\u001b[0m \u001b[0mz_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-66596321219e>\u001b[0m in \u001b[0;36mloss_for_values\u001b[0;34m(m, b)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_for_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"xn9j_1b2nDGJ","colab_type":"text"},"source":["## Polynomial Regression"]},{"cell_type":"markdown","metadata":{"id":"ho1zg0VMnDGK","colab_type":"text"},"source":["In this example we will fit a polynomial using linear regression with a polynomial feature mapping.\n","The target function is:\n","\n","$$\n","y = 0.6x^4 - 1.2 x^2 + 0.8 x + 0.5 + \\epsilon\n","$$\n","\n","where $\\epsilon \\sim \\mathcal{N}(0, 0.001)$. \n","\n","This is an example of a _generalized linear model_, in which we perform a fixed nonlinear transformation of the inputs $\\mathbf{x} = (x_1, x_2, \\dots, x_D)$, and the model is still linear in the _parameters_. We can define a set of _feature mappings_ (also called feature functions or basis functions) $\\phi$ to implement the fixed transformations.\n","\n","In this case, we have $x \\in \\mathbb{R}$, and we define the feature mapping:\n","$$\n","\\mathbf{\\phi}(x) = \\begin{pmatrix}\\phi_1(x) \\\\ \\phi_2(x) \\\\ \\vdots \\\\ \\phi_D(x) \\end{pmatrix} = \\begin{pmatrix}1\\\\x\\\\\\vdots\\\\x^D\\end{pmatrix}\n","$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yRM2qKuh4xLm","colab":{}},"source":["def make_polynomial_data(N=100):\n","  x = tf.random.uniform(shape=(N,))  # TensorFlow and NumPy provide similar functionality\n","  eps = tf.random.normal(shape=(len(x),), stddev=0.01)\n","  y = 0.6* x**4 - 1.2 * x**2 + 0.8 * x + 0.5 + eps\n","  return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_YByGR_T4xLx","colab":{}},"source":["# Generate new polynomial synthetic data\n","x_train, y_train = make_polynomial_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qY4QExMN4xL2","colab":{}},"source":["plt.plot(x_train, y_train, 'r.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kJ4xosBy8iTE","colab":{}},"source":["# Create trainable variables\n","D = 4 # Degree of polynomial to fit to the data (this is a hyperparameter)\n","w = tf.Variable(tf.random.normal((D+1,)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8_Ktpg6W8iTR","colab":{}},"source":["# Predict with feature mapping\n","def predict_poly(x):\n","  pred = w[0] * tf.ones_like(x) # fetch shape of x and create a tensor of ones\n","  for idx in range(1, D+1):\n","    pred = tf.add(pred, w[idx] * tf.pow(x, idx))\n","  return pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LzBUcsZb8iTX","colab":{}},"source":["# Redefine loss function\n","def squared_error_poly(y_pred, y_true):\n","  return tf.reduce_mean(tf.square(y_pred - y_true)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M9KyjWJ68iTb","colab":{}},"source":["# Calculate loss\n","loss = squared_error_poly(predict_poly(x_train), y_train)\n","print(\"Starting loss\", loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XxQ7Q1yN-dhZ","colab":{}},"source":["# Perform gradient descent for vector w\n","alpha = 0.5  # learning rate\n","steps = 10000\n","\n","for i in range(steps):\n","  # GradientTape fetches all variables needed for gradient calculation\n","  # Discussed in Part II\n","  with tf.GradientTape() as tape:\n","    predictions = predict_poly(x_train)\n","    loss = squared_error_poly(predictions, y_train)\n","    \n","  # Calculate gradients via tape\n","  gradients = tape.gradient(loss, [w])\n","  \n","  # Manual gradient descent of the weight vector\n","  w.assign_sub(gradients[0] * alpha)\n","  \n","  if i % 1000 == 0:    \n","    print(f\"Step {i}, Loss {loss.numpy()}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cudU800sEK2I","colab":{}},"source":["print(f'w manual:    {[0.5, 0.8, -1.2, 0.0, 0.6]}')\n","print(f\"w automatic: {w.numpy()}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KiTpR5IhEK2T"},"source":["Plot the best fit line."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wlTq4sCfEK2U","colab":{}},"source":["plt.plot(x_train, y_train, 'r.')\n","plt.plot(x_train, predict_poly(x_train), 'b.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7pC6MU5Oz2m","colab_type":"text"},"source":["It would also be possible to perform Regression by a Neural Network in the same way. One would need to implement a Stochastic Gradient Descent and apply the gradients to each respective variable. As you will see, TensorFlow and Keras provide this functionality, making it easy to train complex networks."]}]}